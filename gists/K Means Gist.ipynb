{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1vaQxYxgjcvaMDV+/9qoN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtRfDlTMGu_5"
      },
      "outputs": [],
      "source": [
        "\n",
        "  '''LIBRARY IMPORTING'''\n",
        "\n",
        "  ## DATA MANAGEMENT\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "from collections import defaultdict\n",
        "\n",
        "  ## DATA VISUALIZATION\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "  ## EVALUATION\n",
        "from time import time\n",
        "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
        "\n",
        "  ## NATURAL LANGUAGE PROCESSING\n",
        "#import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn import metrics\n",
        "\n",
        "  ## Specific to K-Means:\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "  ## Specific to Affinity Propagation:\n",
        "from sklearn.cluster import AffinityPropagation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" DATASET \"\"\"\n",
        "\n",
        "## These are some of the categories in the dataset, feel free to change to different categories outlined in the documentation!\n",
        "categories = [\n",
        "    'rec.autos',\n",
        "    'sci.electronics',\n",
        "    'talk.religion.misc',\n",
        "    'talk.politics.misc'\n",
        "]\n",
        "\n",
        "  ## The documentation page for the dataset suggests removing headers, footers, and quotes to strip and metadata.\n",
        "dataset = fetch_20newsgroups(\n",
        "    remove= ('headers','footers','quotes'),\n",
        "    subset = 'all',\n",
        "    categories=categories,\n",
        "    shuffle = True,\n",
        "    random_state =10,\n",
        ")\n",
        "\n",
        "## generating labels and a true_k to pass into the K Means function\n",
        "labels = dataset.target\n",
        "unique_labels,category_sizes = np.unique(labels,return_counts=True)\n",
        "true_k = unique_labels.shape[0]\n",
        "\n",
        "print(f\"{len(dataset.data)} documents - {true_k} categories\")"
      ],
      "metadata": {
        "id": "g2GC7Q9_Htbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" INSTANTIATING A K-MEANS CLUSTER \"\"\"\n",
        "## First, let's call the TfIDF Vectorizer, and inspect its output\n",
        "\n",
        "  vectorizer = TfidfVectorizer(max_df = 0.5, min_df = 5, stop_words=\"english\")\n",
        "\n",
        "  X_tfidf = vectorizer.fit_transform(dataset.data)\n",
        "\n",
        "## Intalizing storage containers\n",
        "d = collections.defaultdict(list)\n",
        "evaluations = []\n",
        "evaluations_std = []"
      ],
      "metadata": {
        "id": "HLSe01qBHVak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" CREATING A FUNCTION \"\"\"\n",
        "\n",
        "## Defining the function which will both fit and evaluate at the same time.\n",
        "## The parameters for this function are a kmeans object,\n",
        "##  X labels, a name for the evaluation, and amount of training iterations\n",
        "def fit_and_evaluate(km,X,name=None,n_runs=5):\n",
        "\n",
        "    ## Storing the iteration name\n",
        "  name=km.__class__.name__  if name is None else name\n",
        "\n",
        "  ## Creating a storage container for training iterations and scores\n",
        "  train_times = []\n",
        "  scores = d\n",
        "\n",
        "    ## Iterating through each random state to test different iterations of the model\n",
        "  for seed in range(n_runs):\n",
        "    km.set_params(random_state=seed)\n",
        "    t0 = time()\n",
        "    km.fit(X)\n",
        "    train_times.append(time() - t0)\n",
        "\n",
        "    ## Assessing the model for each iteration on performance metrics.\n",
        "    scores['Homogeneity'].append(metrics.homogeneity_score(labels, km.labels_))\n",
        "    scores['Completeness'].append(metrics.completeness_score(labels,km.labels_))\n",
        "    scores['V-measure'].append(metrics.v_measure_score(labels, km.labels_))\n",
        "    scores[\"Adjusted Rand-Index\"].append(metrics.adjusted_rand_score(labels, km.labels_))\n",
        "    scores['Silhouette Coefficient'].append(metrics.silhouette_score(X, km.labels_,sample_size = 2000))\n",
        "\n",
        "\n",
        "    ## Converting the stored model training times to an array.\n",
        "  train_times = np.asarray(train_times)\n",
        "\n",
        "    ## Printing the different iterations training times\n",
        "  print (f\"clustering done in {train_times.mean():.2f} ± {train_times.std():.2f} s\")\n",
        "\n",
        "    ## Storing the evaluation and the evaluation standard deviation to later hold the mean score and STD foreach iteration.\n",
        "  evaluation = {\n",
        "      \"estimator\":name,\n",
        "      \"train_time\": train_times.mean(),\n",
        "  }\n",
        "  evaluation_std = {\n",
        "      \"estimator\":name,\n",
        "      \"train_time\":train_times.std(),\n",
        "  }\n",
        "\n",
        "    ## Parsing through all of the score values to generate a mean and a standard deviation for the centroids.\n",
        "  for score_name, score_values in scores.items():\n",
        "    mean_score, std_score = np.mean(score_values), np.std(score_values)\n",
        "    print(f\"{score_name}: {mean_score:.3f} ± {std_score:.3f}\")\n",
        "    evaluation[score_name] = mean_score\n",
        "    evaluation_std[score_name] = std_score\n",
        "\n",
        "  evaluations.append(evaluation)\n",
        "  evaluations_std.append(evaluation_std)"
      ],
      "metadata": {
        "id": "ifM2EwKFJDGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" CLUSTERING THE TEXT \"\"\"\n",
        "\n",
        "## Defining a for loop to iterate five times and fir a new model each time.\n",
        "for seed in range(5):\n",
        "\n",
        "## Establishing a kmeans object with the unique labels identified earlier with a max iteratino of fifty, and the model will only run once with the random state as the seed.\n",
        "  kmeans_obj=KMeans( n_clusters = true_k, max_iter=50, n_init = 1, random_state=seed)\n",
        "\n",
        "  ## Fitting the KMeans object to the TF-IDf vectorized data\n",
        "  kmeans = kmeans_obj.fit(X_tfidf)\n",
        "\n",
        "  ## Assinging the outputted array to the cluster ids and the cluster sizes.\n",
        "  cluster_ids, cluster_sizes = np.unique(kmeans.labels_, return_counts=True)\n",
        "\n",
        "  ## Printing out the number of elements and the actual class labels for each of the four news categories.\n",
        "  print(f\"Number of elements assigned to each cluster: {cluster_sizes}\")\n",
        "print ()\n",
        "print (\"The true number of documents in each category according to the class labels\"f\"{category_sizes}\")\n",
        "\n",
        "\"\"\" NORMALIZING USING LATENT SEMANTIC ANALYSIS \"\"\"\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "## Assinging the acronym for LSA the truncated SVD role\n",
        "## The pipeline constructs an object which holds the variables\n",
        "lsa = make_pipeline(TruncatedSVD(n_components=100))\n",
        "\n",
        "## Fitting the LSA model\n",
        "t0 = time()\n",
        "X_lsa = lsa.fit_transform(X_tfidf)\n",
        "explained_variance = lsa[0].explained_variance_ratio_.sum()\n",
        "\n",
        "\n",
        "print (f\"LSA done in {time() - t0:.3f} s\")\n",
        "print (f\"Explained variance of the SVD step: {explained_variance * 100:.1f}%\")\n",
        "\n",
        "## Storing the cluster centers and the labels to use later for graphing\n",
        "lsa_cluster_centers = kmeans.cluster_centers_\n",
        "lsa_kmeans_labels = pairwise_distances_argmin(X_lsa, lsa_cluster_centers)\n"
      ],
      "metadata": {
        "id": "eqAKaXYqJNj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" EVALUATING THE CLUSTERS \"\"\"\n",
        " ## Non Normalized\n",
        "fit_and_evaluate(kmeans, X_tfidf, name=\"KMeans\\tf-idf vectors\")\n",
        "\n",
        "  ## LSA Normalized\n",
        "kmeans = KMeans(\n",
        "    n_clusters=true_k,\n",
        "    max_iter=100,\n",
        "    n_init=1\n",
        ")\n",
        "\n",
        "fit_and_evaluate(kmeans,X_lsa,name='KMeans\\nwith LSA on tf-idf vectors')\n",
        "\n"
      ],
      "metadata": {
        "id": "TMo-Y4REJgNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" VIEWING CLUSTER CONTENT \"\"\"\n",
        "from tempfile import TemporaryDirectory\n",
        "original_space_centroids = lsa[0].inverse_transform(kmeans.cluster_centers_)\n",
        "order_centroids = original_space_centroids.argsort()[:,::-1]\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "cluster_top5 = []\n",
        "for i in range(true_k):\n",
        "  print (f\"Cluster {i}: \", end=\"\")\n",
        "\n",
        "  temp_wordlist = []\n",
        "  for ind in order_centroids[i, :7]:\n",
        "    print (f\"{terms[ind]} \",end=\"\")\n",
        "    temp_wordlist.append(terms[ind])\n",
        "\n",
        "  cluster_top5.append(temp_wordlist)\n",
        "  print ()"
      ],
      "metadata": {
        "id": "Yg-trHCwJ4cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "colors = [\"#D52941\", \"#4D9DE0\", \"#FFB20F\",\"#464D77\"]\n",
        "\n",
        "## iterating through each cluster and extracting the k value and closest y\n",
        "for k, col in zip(range(0,4), colors):\n",
        "\n",
        "  ## identify current cluster members if the label value is equivalent to k\n",
        "  curr_members = lsa_kmeans_labels == k\n",
        "\n",
        "  ## extracting the cluster_centers\n",
        "  cluster_center = lsa_cluster_centers[k]\n",
        "\n",
        "  ## plotting the clusters, curr_members 0 and 1 are the x and y values from the array,\n",
        "  ## markerfacecolor sets the clusters color, and \"w\" and \".\" are stylistic.\n",
        "  ax.plot(X_lsa[curr_members, 0], X_lsa[curr_members,1], \"o\", markerfacecolor=col,markeredgecolor='w',markersize=8)\n",
        "\n",
        "  ## plotting the cluster centers for each column with x and y values with other stylized parameters\n",
        "\n",
        "\n",
        "## plotting the cluster centers and text last:\n",
        "\n",
        "for k, col in zip(range(0,4),colors):\n",
        "\n",
        "  cluster_center = lsa_cluster_centers[k]\n",
        "\n",
        "  word_str = \"\"\n",
        "  for word in cluster_top5[k]:\n",
        "    word_str = word_str +word+\" \"\n",
        "\n",
        "  ax.text(cluster_center[0]+.025,cluster_center[1],word_str,fontsize=12,color=\"k\",weight='bold')\n",
        "\n",
        "  ax.plot( cluster_center[0], cluster_center[1], \"X\",\n",
        "      markerfacecolor = col,\n",
        "      markeredgecolor = \"k\",\n",
        "      markersize = 25\n",
        "  )\n",
        "\n",
        "  ## setting plot metadata\n",
        "ax.set_title(\"News Group Clustering\")\n",
        "ax.set_xticks(())\n",
        "ax.set_yticks(());"
      ],
      "metadata": {
        "id": "jz5u1cCrJ_lO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
